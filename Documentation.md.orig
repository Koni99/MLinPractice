<<<<<<< HEAD
# Documentation

## Preprocessing
**Lower case**
- turns every word in the tweet into lower case. Otherwise our classifier would think of e.g. "Dog" and "dog" as two completly different words.

## Feature Extraction
**Sentiment Analysis**
- added compound sentiment score as a feature. We argue that the sentiment of a tweet has an influence on its virality. 
- we used nltk.VADER which is specifically attuned to sentiments expressed in social media
- our function returns a score between -1 very negative and 1 very positive.
- our hypothesis is that tweets that are in the outer range either very negative or very positive will be more popular than neutral tweets with scores around 0.
**HashtagCounter**
-count number of hashtags 
- we assume that tweets with many hashtags attract more attention than tweets without hashtags.
## Dimensionality Reduction

## Classification

### Evaluation metrics
**Accuracy**
- total number of correct predictions divided by total number of predictions in the data set
- default implementation but inappropriate due to imbalanced data set

**Balanced Accuracy**
- arithmetic mean of precision (number of positive class predictions that actually belong to the positive class) and recall (positive class predictions made out of all positive examples)
- variant of accuracy metric that is more appropriate for imbalanced data sets

**F1 Score**
- balances precision and recall in a single number
- f1 considers false positives and false negatives as equally important
- appropriate for imbalanced data sets
- generally better scoring metric than *balanced accuracy* for imbalanced data when more attention on the positives is needed (in our case: tweets predicted as viral)

**Cohen's Kappa**
- measures interrater reliability (reliability for two raters that are rating the same thing, corrected for how often that the raters may agree by chance)
- appropriate for imbalanced data sets

We chose these evaluation metrics due to their respective properties, because they are good to use in our highly imbalanced data set (5% viral 95% not)

### Evaluation baseline
**Majority vote classifier**
- always predicts the majority class (in our case "not viral")

**Uniform distribution classifier**
- generates predictions uniformly at random

**Frequency classifier**
- generates random predictions respecting the training set class distribution (i.e. the label frequency)

## Application
=======
# Documentation Example

Some introductory sentence(s). Data set and task are relatively fixed, so 
probably you don't have much to say about them (unless you modifed them).
If you haven't changed the application much, there's also not much to say about
that.
The following structure thus only covers preprocessing, feature extraction,
dimensionality reduction, classification, and evaluation.

## Evaluation

### Design Decisions

Which evaluation metrics did you use and why? 
Which baselines did you use and why?

### Results

How do the baselines perform with respect to the evaluation metrics?

### Interpretation

Is there anything we can learn from these results?

## Preprocessing

I'm following the "Design Decisions - Results - Interpretation" structure here,
but you can also just use one subheading per preprocessing step to organize
things (depending on what you do, that may be better structured).

### Design Decisions

Which kind of preprocessing steps did you implement? Why are they necessary
and/or useful down the road?

### Results

Maybe show a short example what your preprocessing does.

### Interpretation

Probably, no real interpretation possible, so feel free to leave this section out.

## Feature Extraction

Again, either structure among decision-result-interpretation or based on feature,
up to you.

### Design Decisions

Which features did you implement? What's their motivation and how are they computed?

### Results

Can you say something about how the feature values are distributed? Maybe show some plots?

### Interpretation

Can we already guess which features may be more useful than others?

## Dimensionality Reduction

If you didn't use any because you have only few features, just state that here.
In that case, you can nevertheless apply some dimensionality reduction in order
to analyze how helpful the individual features are during classification

### Design Decisions

Which dimensionality reduction technique(s) did you pick and why?

### Results

Which features were selected / created? Do you have any scores to report?

### Interpretation

Can we somehow make sense of the dimensionality reduction results?
Which features are the most important ones and why may that be the case?

## Classification

### Design Decisions

Which classifier(s) did you use? Which hyperparameter(s) (with their respective
candidate values) did you look at? What were your reasons for this?

### Results

The big finale begins: What are the evaluation results you obtained with your
classifiers in the different setups? Do you overfit or underfit? For the best
selected setup: How well does it generalize to the test set?

### Interpretation

Which hyperparameter settings are how important for the results?
How good are we? Can this be used in practice or are we still too bad?
Anything else we may have learned?
>>>>>>> old_version_test_merge
